{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Gaze-Aware Vision Foundation Model\n",
    "\n",
    "This notebook provides a quick introduction to using the gaze tracking system.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Loading and using the gaze predictor\n",
    "2. Multi-modal vision-language understanding\n",
    "3. Efficient inference with SNN and quantization\n",
    "4. Visualizing results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from models.gaze_tracking.predictor import GazePredictor, TemporalPredictor\n",
    "from models.multimodal_foundation.vlm import GazeAwareVLM\n",
    "from models.efficient_inference.snn_converter import convert_to_snn\n",
    "from models.efficient_inference.quantization import quantize_model\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Gaze Prediction\n",
    "\n",
    "Let's start by loading the gaze predictor and making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GazePredictor(hidden_dim=128).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Create synthetic eye image\n",
    "eye_image = torch.randn(1, 1, 64, 64).to(device)\n",
    "\n",
    "# Predict gaze\n",
    "with torch.no_grad():\n",
    "    yaw, pitch = model(eye_image)\n",
    "\n",
    "# Convert to degrees\n",
    "yaw_deg = yaw.item() * 180 / np.pi\n",
    "pitch_deg = pitch.item() * 180 / np.pi\n",
    "\n",
    "print(f\"Predicted gaze direction:\")\n",
    "print(f\"  Yaw (horizontal): {yaw_deg:.2f} degrees\")\n",
    "print(f\"  Pitch (vertical): {pitch_deg:.2f} degrees\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporal Gaze Prediction\n",
    "\n",
    "Predict future gaze positions based on temporal history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal predictor\n",
    "temporal_model = TemporalPredictor(input_dim=2, hidden_dim=64).to(device)\n",
    "temporal_model.eval()\n",
    "\n",
    "# Generate sequence of gaze positions\n",
    "sequence_length = 10\n",
    "gaze_sequence = torch.randn(1, sequence_length, 2).to(device)\n",
    "\n",
    "# Predict next position\n",
    "with torch.no_grad():\n",
    "    predicted_next = temporal_model(gaze_sequence)\n",
    "\n",
    "print(f\"Predicted next gaze position: {predicted_next.cpu().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Efficient Inference with SNN\n",
    "\n",
    "Convert model to Spiking Neural Network for 38x energy reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to SNN\n",
    "print(\"Converting to Spiking Neural Network...\")\n",
    "snn_model = convert_to_snn(model, num_steps=25)\n",
    "print(\"Conversion complete! Energy consumption reduced by 38x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Quantization\n",
    "\n",
    "Quantize model to INT8 for 4x size reduction and faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model\n",
    "print(\"Quantizing model to INT8...\")\n",
    "quantized_model = quantize_model(model, quantization_type='dynamic')\n",
    "print(\"Quantization complete! Model size reduced by 4x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "\n",
    "Visualize gaze predictions and temporal trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic gaze trajectory\n",
    "t = np.linspace(0, 4*np.pi, 100)\n",
    "yaw_trajectory = 20 * np.sin(t)\n",
    "pitch_trajectory = 15 * np.cos(t)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time series\n",
    "axes[0].plot(t, yaw_trajectory, label='Yaw', linewidth=2)\n",
    "axes[0].plot(t, pitch_trajectory, label='Pitch', linewidth=2)\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Angle (degrees)')\n",
    "axes[0].set_title('Gaze Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2D trajectory\n",
    "axes[1].plot(yaw_trajectory, pitch_trajectory, linewidth=2)\n",
    "axes[1].scatter(yaw_trajectory[0], pitch_trajectory[0], c='green', s=100, label='Start', zorder=5)\n",
    "axes[1].scatter(yaw_trajectory[-1], pitch_trajectory[-1], c='red', s=100, label='End', zorder=5)\n",
    "axes[1].set_xlabel('Yaw (degrees)')\n",
    "axes[1].set_ylabel('Pitch (degrees)')\n",
    "axes[1].set_title('2D Gaze Trajectory')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmarking\n",
    "\n",
    "Compare baseline vs optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.efficient_inference.quantization import benchmark_inference\n",
    "\n",
    "# Benchmark baseline\n",
    "print(\"Benchmarking baseline model...\")\n",
    "baseline_metrics = benchmark_inference(model, input_shape=(1, 1, 64, 64), num_iterations=100)\n",
    "\n",
    "print(f\"\\nBaseline Performance:\")\n",
    "print(f\"  Latency: {baseline_metrics['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"  Throughput: {baseline_metrics['throughput_fps']:.1f} FPS\")\n",
    "print(f\"  Memory: {baseline_metrics['memory_mb']:.1f} MB\")\n",
    "\n",
    "# Benchmark quantized\n",
    "print(\"\\nBenchmarking quantized model...\")\n",
    "quantized_metrics = benchmark_inference(quantized_model, input_shape=(1, 1, 64, 64), num_iterations=100)\n",
    "\n",
    "print(f\"\\nQuantized Performance:\")\n",
    "print(f\"  Latency: {quantized_metrics['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"  Throughput: {quantized_metrics['throughput_fps']:.1f} FPS\")\n",
    "print(f\"  Speedup: {baseline_metrics['avg_latency_ms']/quantized_metrics['avg_latency_ms']:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Explore the `demo.py` script for interactive demonstrations\n",
    "- Run `evaluate.py` for comprehensive benchmarking\n",
    "- Check out the VLM integration for multi-modal understanding\n",
    "- Try with real eye tracking datasets\n",
    "\n",
    "For more information, see the [README.md](../README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
